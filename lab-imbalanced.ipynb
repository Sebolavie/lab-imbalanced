{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB | Imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**\n",
    "\n",
    "In this challenge, we will be working with Credit Card Fraud dataset.\n",
    "\n",
    "https://raw.githubusercontent.com/data-bootcamp-v4/data/main/card_transdata.csv\n",
    "\n",
    "Metadata\n",
    "\n",
    "- **distance_from_home:** the distance from home where the transaction happened.\n",
    "- **distance_from_last_transaction:** the distance from last transaction happened.\n",
    "- **ratio_to_median_purchase_price:** Ratio of purchased price transaction to median purchase price.\n",
    "- **repeat_retailer:** Is the transaction happened from same retailer.\n",
    "- **used_chip:** Is the transaction through chip (credit card).\n",
    "- **used_pin_number:** Is the transaction happened by using PIN number.\n",
    "- **online_order:** Is the transaction an online order.\n",
    "- **fraud:** Is the transaction fraudulent. **0=legit** -  **1=fraud**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57.877857</td>\n",
       "      <td>0.311140</td>\n",
       "      <td>1.945940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.829943</td>\n",
       "      <td>0.175592</td>\n",
       "      <td>1.294219</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.091079</td>\n",
       "      <td>0.805153</td>\n",
       "      <td>0.427715</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.247564</td>\n",
       "      <td>5.600044</td>\n",
       "      <td>0.362663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.190936</td>\n",
       "      <td>0.566486</td>\n",
       "      <td>2.222767</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   distance_from_home  distance_from_last_transaction  \\\n",
       "0           57.877857                        0.311140   \n",
       "1           10.829943                        0.175592   \n",
       "2            5.091079                        0.805153   \n",
       "3            2.247564                        5.600044   \n",
       "4           44.190936                        0.566486   \n",
       "\n",
       "   ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
       "0                        1.945940              1.0        1.0   \n",
       "1                        1.294219              1.0        0.0   \n",
       "2                        0.427715              1.0        0.0   \n",
       "3                        0.362663              1.0        1.0   \n",
       "4                        2.222767              1.0        1.0   \n",
       "\n",
       "   used_pin_number  online_order  fraud  \n",
       "0              0.0           0.0    0.0  \n",
       "1              0.0           0.0    0.0  \n",
       "2              0.0           1.0    0.0  \n",
       "3              0.0           1.0    0.0  \n",
       "4              0.0           1.0    0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/card_transdata.csv\")\n",
    "fraud.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1.** What is the distribution of our target variable? Can we say we're dealing with an imbalanced dataset?\n",
    "- **2.** Train a LogisticRegression.\n",
    "- **3.** Evaluate your model. Take in consideration class importance, and evaluate it by selection the correct metric.\n",
    "- **4.** Run **Oversample** in order to balance our target variable and repeat the steps above, now with balanced data. Does it improve the performance of our model? \n",
    "- **5.** Now, run **Undersample** in order to balance our target variable and repeat the steps above (1-3), now with balanced data. Does it improve the performance of our model?\n",
    "- **6.** Finally, run **SMOTE** in order to balance our target variable and repeat the steps above (1-3), now with balanced data. Does it improve the performance of our model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "features = fraud.drop(columns = [\"fraud\"])\n",
    "target = fraud[\"fraud\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9586"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.99      0.98    228117\n",
      "         1.0       0.89      0.60      0.72     21883\n",
      "\n",
      "    accuracy                           0.96    250000\n",
      "   macro avg       0.93      0.80      0.85    250000\n",
      "weighted avg       0.96      0.96      0.95    250000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = log_reg.predict(X_test_scaled)\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Oversampling \n",
    "train = pd.DataFrame(X_train_scaled, columns = X_train.columns)\n",
    "train[\"fraud\"] = y_train.values\n",
    "\n",
    "fraud = train[train[\"fraud\"] == 1]\n",
    "no_fraud = train[train[\"fraud\"] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_fraud_oversampled = resample(fraud, \n",
    "                                    replace=True, \n",
    "                                    n_samples = len(no_fraud),\n",
    "                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30748</th>\n",
       "      <td>-0.407507</td>\n",
       "      <td>-0.186105</td>\n",
       "      <td>1.664347</td>\n",
       "      <td>-2.724100</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498729</th>\n",
       "      <td>0.260143</td>\n",
       "      <td>0.633002</td>\n",
       "      <td>2.142195</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487954</th>\n",
       "      <td>-0.343323</td>\n",
       "      <td>0.216543</td>\n",
       "      <td>1.171570</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600306</th>\n",
       "      <td>-0.150728</td>\n",
       "      <td>1.868913</td>\n",
       "      <td>0.797743</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525630</th>\n",
       "      <td>1.666520</td>\n",
       "      <td>-0.139132</td>\n",
       "      <td>1.080115</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749995</th>\n",
       "      <td>-0.406291</td>\n",
       "      <td>-0.092952</td>\n",
       "      <td>0.336496</td>\n",
       "      <td>-2.724100</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749996</th>\n",
       "      <td>0.213205</td>\n",
       "      <td>-0.186288</td>\n",
       "      <td>0.400063</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>1.362012</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>-1.362028</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749997</th>\n",
       "      <td>-0.353627</td>\n",
       "      <td>-0.086354</td>\n",
       "      <td>0.035843</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749998</th>\n",
       "      <td>0.381296</td>\n",
       "      <td>-0.091600</td>\n",
       "      <td>0.496381</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749999</th>\n",
       "      <td>-0.021179</td>\n",
       "      <td>-0.183261</td>\n",
       "      <td>-0.463309</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>1.362012</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1368960 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        distance_from_home  distance_from_last_transaction  \\\n",
       "30748            -0.407507                       -0.186105   \n",
       "498729            0.260143                        0.633002   \n",
       "487954           -0.343323                        0.216543   \n",
       "600306           -0.150728                        1.868913   \n",
       "525630            1.666520                       -0.139132   \n",
       "...                    ...                             ...   \n",
       "749995           -0.406291                       -0.092952   \n",
       "749996            0.213205                       -0.186288   \n",
       "749997           -0.353627                       -0.086354   \n",
       "749998            0.381296                       -0.091600   \n",
       "749999           -0.021179                       -0.183261   \n",
       "\n",
       "        ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
       "30748                         1.664347        -2.724100  -0.734208   \n",
       "498729                        2.142195         0.367094  -0.734208   \n",
       "487954                        1.171570         0.367094  -0.734208   \n",
       "600306                        0.797743         0.367094  -0.734208   \n",
       "525630                        1.080115         0.367094  -0.734208   \n",
       "...                                ...              ...        ...   \n",
       "749995                        0.336496        -2.724100  -0.734208   \n",
       "749996                        0.400063         0.367094   1.362012   \n",
       "749997                        0.035843         0.367094  -0.734208   \n",
       "749998                        0.496381         0.367094  -0.734208   \n",
       "749999                       -0.463309         0.367094   1.362012   \n",
       "\n",
       "        used_pin_number  online_order  fraud  \n",
       "30748         -0.334746      0.734199    1.0  \n",
       "498729        -0.334746      0.734199    1.0  \n",
       "487954        -0.334746      0.734199    1.0  \n",
       "600306        -0.334746      0.734199    1.0  \n",
       "525630        -0.334746      0.734199    1.0  \n",
       "...                 ...           ...    ...  \n",
       "749995        -0.334746      0.734199    0.0  \n",
       "749996        -0.334746     -1.362028    0.0  \n",
       "749997        -0.334746      0.734199    0.0  \n",
       "749998        -0.334746      0.734199    0.0  \n",
       "749999        -0.334746      0.734199    0.0  \n",
       "\n",
       "[1368960 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_over = pd.concat([yes_fraud_oversampled, no_fraud])\n",
    "train_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_counts.plot(kind=\"bar\")\n",
    "plt.xlabel(\"Fraud\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Fraud vs Non-Fraud Counts\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_over = train_over.drop(columns = [\"fraud\"])\n",
    "y_train_over = train_over[\"fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression()\n",
    "\n",
    "\n",
    "log_reg.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.93      0.96    228117\n",
      "         1.0       0.58      0.95      0.72     21883\n",
      "\n",
      "    accuracy                           0.93    250000\n",
      "   macro avg       0.79      0.94      0.84    250000\n",
      "weighted avg       0.96      0.93      0.94    250000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = log_reg.predict(X_test_scaled)\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.403357</td>\n",
       "      <td>-0.183434</td>\n",
       "      <td>-0.526205</td>\n",
       "      <td>-2.724100</td>\n",
       "      <td>1.362012</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>-1.362028</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.072548</td>\n",
       "      <td>-0.085925</td>\n",
       "      <td>-0.600340</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>-1.362028</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.367989</td>\n",
       "      <td>-0.164414</td>\n",
       "      <td>-0.182782</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>-1.362028</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.280752</td>\n",
       "      <td>-0.155587</td>\n",
       "      <td>1.049303</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>2.987336</td>\n",
       "      <td>-1.362028</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.277228</td>\n",
       "      <td>-0.162343</td>\n",
       "      <td>-0.562591</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749995</th>\n",
       "      <td>-0.406291</td>\n",
       "      <td>-0.092952</td>\n",
       "      <td>0.336496</td>\n",
       "      <td>-2.724100</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749996</th>\n",
       "      <td>0.213205</td>\n",
       "      <td>-0.186288</td>\n",
       "      <td>0.400063</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>1.362012</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>-1.362028</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749997</th>\n",
       "      <td>-0.353627</td>\n",
       "      <td>-0.086354</td>\n",
       "      <td>0.035843</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749998</th>\n",
       "      <td>0.381296</td>\n",
       "      <td>-0.091600</td>\n",
       "      <td>0.496381</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749999</th>\n",
       "      <td>-0.021179</td>\n",
       "      <td>-0.183261</td>\n",
       "      <td>-0.463309</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>1.362012</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>750000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        distance_from_home  distance_from_last_transaction  \\\n",
       "0                -0.403357                       -0.183434   \n",
       "1                 0.072548                       -0.085925   \n",
       "2                -0.367989                       -0.164414   \n",
       "3                -0.280752                       -0.155587   \n",
       "4                -0.277228                       -0.162343   \n",
       "...                    ...                             ...   \n",
       "749995           -0.406291                       -0.092952   \n",
       "749996            0.213205                       -0.186288   \n",
       "749997           -0.353627                       -0.086354   \n",
       "749998            0.381296                       -0.091600   \n",
       "749999           -0.021179                       -0.183261   \n",
       "\n",
       "        ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
       "0                            -0.526205        -2.724100   1.362012   \n",
       "1                            -0.600340         0.367094  -0.734208   \n",
       "2                            -0.182782         0.367094  -0.734208   \n",
       "3                             1.049303         0.367094  -0.734208   \n",
       "4                            -0.562591         0.367094  -0.734208   \n",
       "...                                ...              ...        ...   \n",
       "749995                        0.336496        -2.724100  -0.734208   \n",
       "749996                        0.400063         0.367094   1.362012   \n",
       "749997                        0.035843         0.367094  -0.734208   \n",
       "749998                        0.496381         0.367094  -0.734208   \n",
       "749999                       -0.463309         0.367094   1.362012   \n",
       "\n",
       "        used_pin_number  online_order  fraud  \n",
       "0             -0.334746     -1.362028    0.0  \n",
       "1             -0.334746     -1.362028    0.0  \n",
       "2             -0.334746     -1.362028    0.0  \n",
       "3              2.987336     -1.362028    0.0  \n",
       "4             -0.334746      0.734199    0.0  \n",
       "...                 ...           ...    ...  \n",
       "749995        -0.334746      0.734199    0.0  \n",
       "749996        -0.334746     -1.362028    0.0  \n",
       "749997        -0.334746      0.734199    0.0  \n",
       "749998        -0.334746      0.734199    0.0  \n",
       "749999        -0.334746      0.734199    0.0  \n",
       "\n",
       "[750000 rows x 8 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5 Undersampling \n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>254865</th>\n",
       "      <td>-0.166412</td>\n",
       "      <td>-0.047915</td>\n",
       "      <td>-0.360437</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277622</th>\n",
       "      <td>-0.368728</td>\n",
       "      <td>-0.162131</td>\n",
       "      <td>-0.293422</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>1.362012</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600299</th>\n",
       "      <td>-0.313727</td>\n",
       "      <td>-0.182604</td>\n",
       "      <td>-0.113132</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>1.362012</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331742</th>\n",
       "      <td>-0.381153</td>\n",
       "      <td>-0.177744</td>\n",
       "      <td>-0.403048</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>-1.362028</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625653</th>\n",
       "      <td>-0.295020</td>\n",
       "      <td>0.040735</td>\n",
       "      <td>-0.313685</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>-1.362028</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619032</th>\n",
       "      <td>0.041956</td>\n",
       "      <td>-0.133769</td>\n",
       "      <td>-0.533631</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>1.362012</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475489</th>\n",
       "      <td>0.489777</td>\n",
       "      <td>-0.126919</td>\n",
       "      <td>0.938224</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>1.362012</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>-1.362028</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450887</th>\n",
       "      <td>-0.348495</td>\n",
       "      <td>-0.087282</td>\n",
       "      <td>-0.459873</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608876</th>\n",
       "      <td>0.152862</td>\n",
       "      <td>-0.091307</td>\n",
       "      <td>-0.308152</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>1.362012</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205874</th>\n",
       "      <td>-0.389923</td>\n",
       "      <td>-0.173623</td>\n",
       "      <td>0.522750</td>\n",
       "      <td>-2.724100</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65520 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        distance_from_home  distance_from_last_transaction  \\\n",
       "254865           -0.166412                       -0.047915   \n",
       "277622           -0.368728                       -0.162131   \n",
       "600299           -0.313727                       -0.182604   \n",
       "331742           -0.381153                       -0.177744   \n",
       "625653           -0.295020                        0.040735   \n",
       "...                    ...                             ...   \n",
       "619032            0.041956                       -0.133769   \n",
       "475489            0.489777                       -0.126919   \n",
       "450887           -0.348495                       -0.087282   \n",
       "608876            0.152862                       -0.091307   \n",
       "205874           -0.389923                       -0.173623   \n",
       "\n",
       "        ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
       "254865                       -0.360437         0.367094  -0.734208   \n",
       "277622                       -0.293422         0.367094   1.362012   \n",
       "600299                       -0.113132         0.367094   1.362012   \n",
       "331742                       -0.403048         0.367094  -0.734208   \n",
       "625653                       -0.313685         0.367094  -0.734208   \n",
       "...                                ...              ...        ...   \n",
       "619032                       -0.533631         0.367094   1.362012   \n",
       "475489                        0.938224         0.367094   1.362012   \n",
       "450887                       -0.459873         0.367094  -0.734208   \n",
       "608876                       -0.308152         0.367094   1.362012   \n",
       "205874                        0.522750        -2.724100  -0.734208   \n",
       "\n",
       "        used_pin_number  online_order  fraud  \n",
       "254865        -0.334746      0.734199    0.0  \n",
       "277622        -0.334746      0.734199    0.0  \n",
       "600299        -0.334746      0.734199    0.0  \n",
       "331742        -0.334746     -1.362028    0.0  \n",
       "625653        -0.334746     -1.362028    0.0  \n",
       "...                 ...           ...    ...  \n",
       "619032        -0.334746      0.734199    0.0  \n",
       "475489        -0.334746     -1.362028    0.0  \n",
       "450887        -0.334746      0.734199    0.0  \n",
       "608876        -0.334746      0.734199    0.0  \n",
       "205874        -0.334746      0.734199    0.0  \n",
       "\n",
       "[65520 rows x 8 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_fraud_undersampled = resample(no_fraud, \n",
    "                                    replace=False, \n",
    "                                    n_samples = len(fraud),\n",
    "                                    random_state=0)\n",
    "no_fraud_undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>254865</th>\n",
       "      <td>-0.166412</td>\n",
       "      <td>-0.047915</td>\n",
       "      <td>-0.360437</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277622</th>\n",
       "      <td>-0.368728</td>\n",
       "      <td>-0.162131</td>\n",
       "      <td>-0.293422</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>1.362012</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600299</th>\n",
       "      <td>-0.313727</td>\n",
       "      <td>-0.182604</td>\n",
       "      <td>-0.113132</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>1.362012</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331742</th>\n",
       "      <td>-0.381153</td>\n",
       "      <td>-0.177744</td>\n",
       "      <td>-0.403048</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>-1.362028</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625653</th>\n",
       "      <td>-0.295020</td>\n",
       "      <td>0.040735</td>\n",
       "      <td>-0.313685</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>-1.362028</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749955</th>\n",
       "      <td>0.132607</td>\n",
       "      <td>0.046506</td>\n",
       "      <td>3.947910</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749961</th>\n",
       "      <td>-0.365756</td>\n",
       "      <td>0.674058</td>\n",
       "      <td>0.818622</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749966</th>\n",
       "      <td>-0.138692</td>\n",
       "      <td>-0.077803</td>\n",
       "      <td>1.232626</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749977</th>\n",
       "      <td>3.802249</td>\n",
       "      <td>-0.183941</td>\n",
       "      <td>-0.244602</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>-0.734208</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749979</th>\n",
       "      <td>-0.343499</td>\n",
       "      <td>0.527632</td>\n",
       "      <td>1.317485</td>\n",
       "      <td>0.367094</td>\n",
       "      <td>1.362012</td>\n",
       "      <td>-0.334746</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131040 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        distance_from_home  distance_from_last_transaction  \\\n",
       "254865           -0.166412                       -0.047915   \n",
       "277622           -0.368728                       -0.162131   \n",
       "600299           -0.313727                       -0.182604   \n",
       "331742           -0.381153                       -0.177744   \n",
       "625653           -0.295020                        0.040735   \n",
       "...                    ...                             ...   \n",
       "749955            0.132607                        0.046506   \n",
       "749961           -0.365756                        0.674058   \n",
       "749966           -0.138692                       -0.077803   \n",
       "749977            3.802249                       -0.183941   \n",
       "749979           -0.343499                        0.527632   \n",
       "\n",
       "        ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
       "254865                       -0.360437         0.367094  -0.734208   \n",
       "277622                       -0.293422         0.367094   1.362012   \n",
       "600299                       -0.113132         0.367094   1.362012   \n",
       "331742                       -0.403048         0.367094  -0.734208   \n",
       "625653                       -0.313685         0.367094  -0.734208   \n",
       "...                                ...              ...        ...   \n",
       "749955                        3.947910         0.367094  -0.734208   \n",
       "749961                        0.818622         0.367094  -0.734208   \n",
       "749966                        1.232626         0.367094  -0.734208   \n",
       "749977                       -0.244602         0.367094  -0.734208   \n",
       "749979                        1.317485         0.367094   1.362012   \n",
       "\n",
       "        used_pin_number  online_order  fraud  \n",
       "254865        -0.334746      0.734199    0.0  \n",
       "277622        -0.334746      0.734199    0.0  \n",
       "600299        -0.334746      0.734199    0.0  \n",
       "331742        -0.334746     -1.362028    0.0  \n",
       "625653        -0.334746     -1.362028    0.0  \n",
       "...                 ...           ...    ...  \n",
       "749955        -0.334746      0.734199    1.0  \n",
       "749961        -0.334746      0.734199    1.0  \n",
       "749966        -0.334746      0.734199    1.0  \n",
       "749977        -0.334746      0.734199    1.0  \n",
       "749979        -0.334746      0.734199    1.0  \n",
       "\n",
       "[131040 rows x 8 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_under = pd.concat([no_fraud_undersampled, fraud])\n",
    "train_under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fraud_plt = train_under[\"fraud\"].value_counts()\n",
    "fraud_plt.plot(kind=\"bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating \n",
    "X_train_under = train_under.drop(columns = [\"fraud\"])\n",
    "y_train_under = train_under[\"fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#modeling\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_under, y_train_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.93      0.96    228117\n",
      "         1.0       0.57      0.95      0.72     21883\n",
      "\n",
      "    accuracy                           0.93    250000\n",
      "   macro avg       0.78      0.94      0.84    250000\n",
      "weighted avg       0.96      0.93      0.94    250000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate\n",
    "pred = log_reg.predict(X_test_scaled)\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 SMOTE baby, yeah \n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state = 1,sampling_strategy=1.0)\n",
    "X_train_sm,y_train_sm = sm.fit_resample(X_train_scaled,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_sm, y_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.93      0.96    228117\n",
      "         1.0       0.58      0.95      0.72     21883\n",
      "\n",
      "    accuracy                           0.93    250000\n",
      "   macro avg       0.79      0.94      0.84    250000\n",
      "weighted avg       0.96      0.93      0.94    250000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate model \n",
    "pred = log_reg.predict(X_test_scaled)\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
